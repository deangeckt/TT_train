{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from utils.general import read_raw_data\n",
    "\n",
    "# keep x and y unchanged\n",
    "x, y, names = read_raw_data(score_path='labels/fts_score.csv', data_path='labels/fts_data.csv')\n",
    "# TODO: code to combine fts and fco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Network\n",
    "NUM_FEATURES = 100\n",
    "HIDDEN_DIM = 128\n",
    "OUTPUT_DIM = 2\n",
    "BATCH_SIZE = 4\n",
    "IS_BID = True\n",
    "NUM_LAYERS = 3\n",
    "\n",
    "## Optimizer  \n",
    "lr = 1e-3\n",
    "weight_decay=0.001\n",
    "\n",
    "## Scheduler\n",
    "step_size=3\n",
    "gamma=0.5\n",
    "\n",
    "early_stop_patience = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def feature_selection(remove_features):\n",
    "    copy = x.clone() # don't change x\n",
    "    # TODO - impl\n",
    "    return copy\n",
    "\n",
    "#Usage example - remove head landmarks \n",
    "head_landmarks = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "data = feature_selection(head_landmarks)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_threshold = 6\n",
    "labels = np.array(y)\n",
    "labels[labels <= score_threshold] = 0\n",
    "labels[labels > score_threshold] = 1\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "for idx, c in enumerate(counts):\n",
    "    print(f'label: {idx}: {c} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Set & Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "def split_(ratio, dataset):\n",
    "    major_size = int(ratio * len(dataset))\n",
    "    minor_size = int((1-ratio) * len(dataset))\n",
    "    return random_split(dataset, [major_size, minor_size + 1], torch.Generator().manual_seed(42))\n",
    "\n",
    "\n",
    "class TTDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "def create_dataloaders(batch_size):\n",
    "    dataset_all = TTDataset(data, labels)\n",
    "    train_data, test_data = split_(0.8, dataset_all)\n",
    "    train_data, validation_data = split_(0.6, train_data)\n",
    "    \n",
    "    print('#train samples: ', len(train_data))\n",
    "    print('#valid samples: ', len(validation_data))\n",
    "    print('#test  samples: ', len(test_data))\n",
    "    \n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    validation_loader = DataLoader(validation_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return train_loader, validation_loader, test_loader\n",
    "\n",
    "    \n",
    "train_loader, validation_loader, test_loader = create_dataloaders(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.scale = 1. / math.sqrt(hidden_size)\n",
    "\n",
    "    def forward(self, hidden, outputs):\n",
    "        hidden = hidden.unsqueeze(1)\n",
    "        values = outputs.transpose(0, 1)\n",
    "        outputs = outputs.transpose(0, 1).transpose(1, 2)\n",
    "\n",
    "        weights = torch.bmm(hidden, outputs)\n",
    "        scores = F.softmax(weights.mul_(self.scale), dim=2)\n",
    "        linear_combination = torch.bmm(scores, values).squeeze(1)\n",
    "        return linear_combination\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, input_dim, h_dim, out_dim, num_layers=1, bidirectional=True):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.h_dim = h_dim\n",
    "        self.bidirectional = bidirectional\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # Attention layer - unused so far\n",
    "        hid_bidirectional = h_dim * 2 if bidirectional else h_dim\n",
    "        self.atten = Attention(hid_bidirectional)\n",
    "\n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(input_size=input_dim,\n",
    "                            hidden_size=h_dim,\n",
    "                            num_layers=num_layers,\n",
    "                            bidirectional=bidirectional, batch_first=True)  # dropout\n",
    "        # FC layer\n",
    "        self.fc = nn.Linear(hid_bidirectional, out_dim, bias=True)\n",
    "\n",
    "        # To convert class scores to log-probability we'll apply log-softmax\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        lay_times_dir = self.num_layers * 2 if self.bidirectional else self.num_layers\n",
    "        h0 = torch.zeros(lay_times_dir, BATCH_SIZE, HIDDEN_DIM).to(device)\n",
    "        c0 = torch.zeros(lay_times_dir, BATCH_SIZE, HIDDEN_DIM).to(device)\n",
    "\n",
    "        out, (h_t, c_t) = self.lstm(X, (h0, c0))\n",
    "        if self.bidirectional:\n",
    "            cell_state = torch.cat([c_t[-1], c_t[-2]], dim=1)\n",
    "        else:\n",
    "            cell_state = c_t[-1]\n",
    "\n",
    "        y = self.fc(cell_state)\n",
    "        yt_log_proba = self.log_softmax(y)\n",
    "        return yt_log_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.learn import train_model\n",
    "\n",
    "model = Network(NUM_FEATURES, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                bidirectional=IS_BID, num_layers=NUM_LAYERS).to(device)\n",
    "model_name = f'b{BATCH_SIZE}_lr{lr}_sz{step_size}_g{gamma}_h{HIDDEN_DIM}_nl{NUM_LAYERS}.pt'\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "loss_fn = nn.NLLLoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "\n",
    "train_model(model, model_name, BATCH_SIZE, device, early_stop_patience,\n",
    "            train_loader, validation_loader, test_loader,\n",
    "            optimizer, scheduler, loss_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# False analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
